{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "import json\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696f3b2",
   "metadata": {},
   "source": [
    "# Car Price Prediction Project  \n",
    "  \n",
    "This project aims to compare nine different machine learning models (Linear Regression, KNN, Decision Tree, Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost, AdaBoost) to predict the `sellingprice` value of used cars.\n",
    "\n",
    "## Step 1: Data Loading and Exploratory Data Analysis (EDA)  \n",
    "  \n",
    "The first phase of the project is to understand the raw data. The `car_prices.csv` file was loaded using the `pandas` library, and the following initial examinations were performed to understand the basic structure of the dataset:\n",
    "\n",
    "1.  **`.head()`**: The first 5 rows of the dataset were examined to observe the column names and general data structure.\n",
    "2.  **`.info()`**: The total number of rows, column data types (Dtype), and the number of non-null values were checked. This allowed us to quickly see which columns had missing data (NaN).\n",
    "3.  **`.isecondsull().sum()`**: The total number of missing values in each column was clearly calculated.\n",
    "4.  **`.describe()`**: The basic statistics (mean, min, max, quartiles) of numerical columns (e.g., `odometer`, `sellingprice`) were examined. This is important for detecting possible outliers or erroneous data (e.g., minimum price £1).\n",
    "5.  **`.describe(include=['object'])`**: The number of unique values, the most frequent category (top), and its frequency (freq) were examined for categorical (text) columns (e.g., `make`, `model`, `transmission`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"car_prices.csv\", on_bad_lines=\"skip\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ded47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b4c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f05047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe(include=[\"object\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b606e",
   "metadata": {},
   "source": [
    "## Step 1.5: Visualising Data  \n",
    "  \n",
    "Before commencing the data cleaning process, it is important to visualise the most critical issues and relationships identified in Step 1 (EDA). At this stage, the distribution of our target variable `sellingprice`, potential data leakage (`mmr`), and the impact of `odometer`—one of the most important features—on price were examined.\n",
    "\n",
    "1.  **`sellingprice` Distribution (Histogram):** It was observed that the price distribution was skewed to the right and that there were very low (erroneous) price records close to 0.\n",
    "2.  **`mmr` vs `sellingprice` (Scatter Plot):** The graph of these two variables showed an almost perfect linear (y=x) relationship. This proves that the `mmr` column acts as an \"answer\" for `sellingprice` and leads to **data leakage**. Therefore, the `mmr` column should not be included in model training.\n",
    "3.  **`odometer` vs `sellingprice` (Scatter Plot):** As expected, a negative relationship was observed, showing that as mileage increases, the selling price (generally) decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['sellingprice'], bins=100, kde=True)\n",
    "plt.title('Selling Price Distribution', fontsize=16)\n",
    "plt.xlabel('Selling Price', fontsize=12)\n",
    "plt.ylabel('Frequency (Number)', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc328de-22e5-4fea-9fec-950c6f1055a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(n=5000, random_state=42)\n",
    "sns.scatterplot(data=sample_df, x='mmr', y='sellingprice', alpha=0.6)\n",
    "\n",
    "max_val = max(sample_df['mmr'].max(), sample_df['sellingprice'].max())\n",
    "min_val = min(sample_df['mmr'].min(), sample_df['sellingprice'].min())\n",
    "\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Market Value (mmr) vs Selling Price', fontsize=16)\n",
    "plt.xlabel('Market Value (mmr)', fontsize=12)\n",
    "plt.ylabel('Selling Price', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc9df9-2caf-4b55-b16e-6394085926ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=sample_df, x='odometer', y='sellingprice', alpha=0.6)\n",
    "plt.title('Kilometre (odometer) vs Selling price', fontsize=16)\n",
    "plt.xlabel('Kilometre (odometer)', fontsize=12)\n",
    "plt.ylabel('Selling Price', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302bd51-7c5d-4757-9361-1236ece28ea9",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Feature Engineering\n",
    "\n",
    "Raw data is not suitable for training models. The following steps were taken to resolve the issues identified in Steps 1 and 1.5:\n",
    "\n",
    "1.  **Removal of Data Leakage and Unnecessary Columns:**\n",
    "    * `mmr`: As demonstrated in Step 1.5, it was **removed** from the dataset because it had a perfect correlation with `sellingprice` and caused data leakage.\n",
    "    * `vin`: It was **removed** because it is a unique identifier (ID) for each record and does not provide a pattern for the model.\n",
    "\n",
    "2.  **Outlier Cleaning:**\n",
    "    * There were obvious erroneous entries such as `$1` in the `sellingprice` column. To prevent the model from being affected by this noise, records with a `sellingprice` value below $100 were **filtered** from the dataset.\n",
    "\n",
    "3.  **Feature Engineering:**\n",
    "    * To help models better understand the concept of \"time,\" a new numerical feature called `car_age` (the age of the car at the time of sale) was **derived** using the `saledate` (object) and `year` (int) columns.\n",
    "    * The `saledate` column was first converted to `datetime` format, and invalid dates were marked as `NaT`.\n",
    "    * The formula `car_age = saledate.year - year` was applied.\n",
    "    * After processing, the `saledate` and `year` columns were **removed** from the dataset.\n",
    "\n",
    "4.  **Missing Value Imputation:**\n",
    "    * **Numeric Columns (`odometer`, `condition`):** Missing values were imputed with the **median** value, which is more robust against outliers.\n",
    "    * **Categorical Columns (`transmission`, `make`, `model`, `body`, `colour`, etc.):** All categorical missing values, including over 65,000 missing `transmission` values, were **filled** with a new category named `\"Unknown\"` so that the model could learn \"information absence\" as a feature.\n",
    "\n",
    "5.  **Result:** As a result of these operations, a cleaned dataset with no missing (NaN) values was obtained and saved as `car_prices_cleaned.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ebc09-405d-4fb0-92c1-46e5261af433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"vin\", \"mmr\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fe55f-8576-42cb-82cf-5a86da29da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"sellingprice\"] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a68965-1de0-4fc9-8be2-3e781b39cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"saledate\"] = pd.to_datetime(df[\"saledate\"], errors=\"coerce\", utc=True)\n",
    "if pd.api.types.is_datetime64tz_dtype(df[\"saledate\"]):\n",
    "\tdf[\"saledate\"] = df[\"saledate\"].dt.tz_convert(None)\n",
    "\n",
    "df = df.dropna(subset=[\"saledate\", \"year\"])\n",
    "df[\"sale_year\"] = df[\"saledate\"].dt.year.astype(int)\n",
    "df[\"car_age\"] = df[\"sale_year\"] - df[\"year\"].astype(int)\n",
    "df = df.drop([\"year\", \"saledate\", \"sale_year\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b497a-2d96-4c5b-8d80-2797364861f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"odometer\", \"condition\"]:\n",
    "    median_val = df[col].median()\n",
    "    df[col] = df[col].fillna(median_val)\n",
    "categorical_cols = ['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].fillna(\"Unknown\")\n",
    "df[\"seller\"] = df[\"seller\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca50fa-aefd-4a43-bdf1-e84adcc6475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows remaining after cleaning: {len(df)}\")\n",
    "print(\"\\n--- Cleaned Data Set Information (info) ---\")\n",
    "df.info()\n",
    "print(\"\\n--- Missing Data Status of the Cleaned Dataset (isecondsull().sum) ---\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n--- First 5 Rows of the Cleaned Data (head) ---\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fd0c8-9c76-4652-9d9a-8d524afcbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"car_prices_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e8b6e-22f0-46e5-864d-e66537f1190a",
   "metadata": {},
   "source": [
    "## Step 3: Advanced Data Transformation (Target Encoding) and Data Splitting\n",
    "\n",
    "Machine learning models cannot process text data (`object`). Our dataset contains columns with thousands of unique categories (high cardinality), such as `model`, `trim`, and `seller`.\n",
    "\n",
    "**Challenge:** Applying `One-Hot Encoding` (`pd.get_dummies`) to these columns would create over 15,000 new columns, leading to the \"Curse of Dimensionality\" and reducing the performance of models such as Linear Regression and KNN.\n",
    "\n",
    "**Solution:** Instead of discarding these columns, an advanced technique called **Target Encoding** was used to increase accuracy.\n",
    "\n",
    "1.  **What is Target Encoding?**\n",
    "    * This technique replaces a categorical value (e.g., `model = 'Sorento'`) with the average target variable corresponding to that category (in our project, the average `sellingprice`).\n",
    "    * This way, instead of 15,000+ columns, we obtain numerical columns containing *very powerful* information about the price.\n",
    "\n",
    "2.  **Preventing Data Leakage:**\n",
    "    * The biggest risk of this technique is data leakage. To prevent this, the dataset was split into 80% Training (`X_train`, `y_train`) and 20% Test (`X_test`, `y_test`) **before any encoding was performed**.\n",
    "    * `TargetEncoder` from the `category_encoders` library was used.\n",
    "* The encoder was \"trained\" (`.fit()`) using **ONLY** the `X_train` and `y_train` data.\n",
    "    * This trained encoder was applied to both the `X_train` and `X_test` data (`.transform()`). This ensured that no test data information leaked into the training phase.\n",
    "\n",
    "3.  **Result:**\n",
    "    * All categorical columns (`make`, `model`, `trim`, `body`, etc.) were successfully converted to numerical (float64) columns.\n",
    "    * The dataset is now ready for the next stage: Scaling and Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aae07-5b68-46ef-b283-81ab3487f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv(\"car_prices_cleaned.csv\")\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83f8a7-bb49-4c6d-8137-88422223306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_clean.drop(\"sellingprice\", axis=1)\n",
    "y = df_clean[\"sellingprice\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c97994-7b7e-4904-b021-1fae793ab373",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = x_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(f\"Target Encoding will be applied to {len(categorical_cols)} columns found: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc19236-67ee-463f-9c56-da7265f62ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle_unknown='value': If a category appears in the test set but not in the training set \n",
    "# (e.g., a new 'model'), assign it the overall average price.\n",
    "encoder = ce.TargetEncoder(cols=categorical_cols, handle_unknown=\"value\")\n",
    "encoder.fit(x_train, y_train)\n",
    "x_train_encoded = encoder.transform(x_train)\n",
    "x_test_encoded = encoder.transform(x_test)\n",
    "x_train_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9211a-7c8f-4686-93f1-4c8b0f70bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded.to_csv(\"x_train_encoded.csv\", index=False)\n",
    "x_test_encoded.to_csv(\"x_test_encoded.csv\", index=False)\n",
    "y_train.to_csv(\"y_train.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6417adc-93db-4062-87a7-d56eb8c2823e",
   "metadata": {},
   "source": [
    "## Step 4: Feature Scaling\n",
    "\n",
    "Although our dataset is now entirely numerical, one final challenge remains: the scales of the features (columns) vary greatly.\n",
    "\n",
    "* `odometer` takes values in the 100,000s, while `condition` takes values between 1 and 5.\n",
    "* Columns such as `make` and `model`, which we created with Target Encoding, represent average prices (e.g., 5,000 - 50,000).\n",
    "\n",
    "**Problem:** Distance and coefficient-based models such as **Linear Regression** and **KNN (K-Nearest Neighbors)** consider the `odometer` column 10,000 times more important than the `condition` column simply because its numerical value is larger.\n",
    "\n",
    "**Solution:** Standardise all features using `StandardScaler` from the `sklearn.preprocessing` library.\n",
    "\n",
    "1.  **What is Standardisation?** It is the process of transforming all features so that their means are 0 (${\\mu}=0$) and their standard deviations are 1 (${\\sigma}=1$).\n",
    "2.  **Preventing Data Leakage:** As with `TargetEncoder`, `StandardScaler` is \"trained\" (`.fit()`) ONLY on the `X_train` data, and this training (calculated mean and standard deviation) is applied to both the `X_train` and `X_test` data (`.transform()`).\n",
    "3.  **Tree-Based Models:** Tree-based models such as Random Forest, XGBoost, and LightGBM are *not affected* by scaling. However, to fairly compare all 9 of our models on the same dataset, using scaled data would be a consistent approach.\n",
    "\n",
    "**Result:** The `X_train_scaled.csv` and `X_test_scaled.csv` files have been created. The data preparation process is complete. Our data is now ready for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5cdf7-f935-4c51-b970-7d2ad3dda015",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train_encoded.csv\")\n",
    "x_test = pd.read_csv(\"x_test_encoded.csv\")\n",
    "# We convert the DataFrame to a Series using .squeeze()\n",
    "y_train = pd.read_csv(\"y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9483b1-37c6-4a3f-af05-16861ad5752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1435b7f-ee69-4818-bd3e-330adf0f1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .transform() returns a numpy array.\n",
    "# To preserve the column names, we convert it back to a DataFrame.\n",
    "x_train_scaled_np = scaler.transform(x_train)\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled_np, columns=x_test.columns)\n",
    "x_test_scaled_np = scaler.transform(x_test)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled_np, columns=x_test.columns)\n",
    "x_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac01133-89df-4efb-b1fa-4fd5b41235a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d543a-510b-4b87-a42a-b8c40a35c0c6",
   "metadata": {},
   "source": [
    "## Step 5: Baseline Model Training and Comparison\n",
    "\n",
    "At this stage, nine different models were trained using **default parameters** and their performance was compared for a fair \"Baseline Comparison\". The aim was to see which models were \"naturally\" more suited to this dataset.\n",
    "\n",
    "The evaluation metrics were set as R² (close to 1 = good), MAE (low = good) and RMSE (low = good).\n",
    "\n",
    "### 5.1 Model 1: Linear Regression\n",
    "\n",
    "Linear Regression is the most basic regression model, attempting to establish a linear relationship between all features and the target (`sellingprice`).\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.7615\n",
    "  - **MAE:** 3092.15 \n",
    "  - **RMSE:** 4780.80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3259086-e083-42c1-be7f-2c38d2c51faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled.to_csv(\"x_train_scaled.csv\", index=False)\n",
    "x_test_scaled.to_csv(\"x_test_scaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed45b6c-b8c4-4994-91c9-f121e4606940",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train_scaled.csv\")\n",
    "x_test = pd.read_csv(\"x_test_scaled.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\").squeeze()\n",
    "y_test = pd.read_csv(\"y_test.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e3ab6-f617-40d9-a3fe-8ca06a0883c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LinearRegression()\n",
    "start_fit = time.time()\n",
    "model_lr.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_lr = model_lr.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "print(\"--- Linear Regression Results ---\")\n",
    "print(f\"R-squared: {r2_lr:.4f}\")\n",
    "print(f\"MAE: {mae_lr:.2f} $\")\n",
    "print(f\"RMSE: {rmse_lr:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457f3f2-a675-4449-a9e1-d707ea320c1f",
   "metadata": {},
   "source": [
    "### 5.2 Model 2: KNN (K-Nearest Neighbors)\n",
    "\n",
    "KNN is a \"distance\"-based model. To predict the price of a car, it takes the average price of the `k` neighbouring cars (from the training data) that most closely resemble that car in the test data in terms of features.\n",
    "\n",
    "The **Feature Scaling** we performed in Step 4 was essential for this model to work. The model was run with the default parameter `k=5`.\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.8842\n",
    "  - **MAE:** 2003.76\n",
    "  - **RMSE:** 3331.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af785c8e-090c-4abe-a773-c50a9da0dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsRegressor(n_jobs=-1)\n",
    "start_fit = time.time()\n",
    "model_knn.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_knn = model_knn.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "rmse_knn = np.sqrt(mse_knn)\n",
    "print(\"\\n--- KNN Results ---\")\n",
    "print(f\"R-squared: {r2_knn:.4f}\")\n",
    "print(f\"MAE: {mae_knn:.2f} $\")\n",
    "print(f\"RMSE: {rmse_knn:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41f82b-94a8-4139-9857-0dd4ba74108c",
   "metadata": {},
   "source": [
    "### 5.3 Model 3: Decision Tree\n",
    "\n",
    "A Decision Tree is a model that works by splitting data according to a series of \"if-then\" rules. When run with default parameters (unlimited depth), it is highly prone to overfitting the training data. This test aims to measure the performance of the model in its natural (raw) state.\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.9077\n",
    "  - **MAE:** 1728.58\n",
    "  - **RMSE:** 2975.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd8596-c62c-46f6-9d4f-9a7e160302b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeRegressor(random_state=7)\n",
    "start_fit = time.time()\n",
    "model_dt.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_dt = model_dt.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "rmse_dt = np.sqrt(mse_dt)\n",
    "print(\"\\n--- Decision Tree Results ---\")\n",
    "print(f\"R-squared: {r2_dt:.4f}\")\n",
    "print(f\"MAE: {mae_dt:.2f} $\")\n",
    "print(f\"RMSE: {rmse_dt:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7d23e-8e68-48ec-8705-623e198612c6",
   "metadata": {},
   "source": [
    "### 5.4 Model 4: Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method designed to solve the biggest problem of a single Decision Tree (Model 5.3), namely **overfitting**.\n",
    "\n",
    "This model builds a \"forest\" consisting of **100** more random and less deep trees instead of a single deep tree. When making a prediction, it takes the average of the predictions of these 100 trees. This approach makes the model much more stable and reliable.\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.9551\n",
    "  - **MAE:** 1220.07\n",
    "  - **RMSE:** 2075.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152aa5a-a164-4259-8b02-8992c6d5b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(random_state=7, n_jobs=-1)\n",
    "start_fit = time.time()\n",
    "model_rf.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_rf = model_rf.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "print(f\"R-squared: {r2_rf:.4f}\")\n",
    "print(f\"MAE: {mae_rf:.2f} $\")\n",
    "print(f\"RMSE: {rmse_rf:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3d7c1-7f8e-439a-a9df-0fc80d72741d",
   "metadata": {},
   "source": [
    "### 5.5 Model 5: Gradient Boosting (GBM)\n",
    "\n",
    "After the \"Bagging\" method used by Random Forest, we test the Gradient Boosting model, the first and most classic member of the \"Boosting\" family.\n",
    "\n",
    "* **Bagging (Random Forest):** 100 independent trees are trained in parallel and the results are averaged (Democracy).\n",
    "* **Boosting (Gradient Boosting):** Trees are trained *sequentially*. Each new tree specialises in correcting the errors made by the previous tree.\n",
    "\n",
    "This \"learning from mistakes\" approach typically produces very powerful and highly accurate models.\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.8700\n",
    "  - **MAE:** 2220.40\n",
    "  - **RMSE:** 3530.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bce8c8-8b50-4892-ae31-25975466e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = GradientBoostingRegressor(random_state=7)\n",
    "start_fit = time.time()\n",
    "model_gb.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_gb = model_gb.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "rmse_gb = np.sqrt(mse_gb)\n",
    "print(\"\\n--- Gradient Boosting Results ---\")\n",
    "print(f\"R-squared: {r2_gb:.4f}\")\n",
    "print(f\"MAE: {mae_gb:.2f} $\")\n",
    "print(f\"RMSE: {rmse_gb:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129b4fd-0213-4c79-b559-8dd086e085d8",
   "metadata": {},
   "source": [
    "### 5.6 Model 6: XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "The `GradientBoostingRegressor` (scikit-learn) in Model 5.5, with its default parameters (MAE: 2161), lagged far behind Random Forest (MAE: 1210). This demonstrates the sensitivity of classic GBM to hyperparameters.\n",
    "\n",
    "XGBoost (\"Extreme Boosting\") is a much faster and more efficient implementation of the same \"boosting\" idea. Predominantly used in data science competitions such as Kaggle, this model is much more advanced than classic GBM in terms of both parallelisation (speed) and regularisation (preventing overfitting).\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.9417\n",
    "  - **MAE:** 1463.40\n",
    "  - **RMSE:** 2363.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ae795-553a-4237-aa6f-4defe7f1e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor(random_state=7, n_jobs=-1)\n",
    "start_fit = time.time()\n",
    "model_xgb.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_xgb = model_xgb.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "print(f\"R-squared: {r2_xgb:.4f}\")\n",
    "print(f\"MAE: {mae_xgb:.2f} $\")\n",
    "print(f\"RMSE: {rmse_xgb:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c407987-e133-4201-929f-28205a8fab1a",
   "metadata": {},
   "source": [
    "### 5.7 Model 7: LightGBM (Light Gradient Boosting Machine) \n",
    " The second member of the modern \"boosting\" family is LightGBM, developed by Microsoft. It is designed to be even faster than XGBoost. \n",
    " While XGBoost grows trees \"level-wise\", LightGBM follows a \"leaf-wise\" strategy that minimises error. This provides a significant speed advantage, particularly with datasets containing hundreds of thousands of rows (like ours).\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.9229\n",
    "  - **MAE:** 1736.79\n",
    "  - **RMSE:** 2718.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5058f59b-a036-4839-9a1d-eb6bfc3a13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm= LGBMRegressor(random_state=7, verbosity=-1, n_jobs=-1)\n",
    "start_fit = time.time()\n",
    "model_lgbm.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_lgbm = model_lgbm.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
    "mae_lgbm = mean_absolute_error(y_test, y_pred_lgbm)\n",
    "mse_lgbm = mean_squared_error(y_test, y_pred_lgbm)\n",
    "rmse_lgbm = np.sqrt(mse_lgbm)\n",
    "print(\"\\n--- LightGBM Results ---\")\n",
    "print(f\"R-squared: {r2_lgbm:.4f}\")\n",
    "print(f\"MAE: {mae_lgbm:.2f} $\")\n",
    "print(f\"RMSE: {rmse_lgbm:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e45184-5a41-40dd-beb0-b1c686e39d9d",
   "metadata": {},
   "source": [
    "### 5.8 Model 8: CatBoost (Categorical Boosting)\n",
    "\n",
    "The latest member of the modern \"boosting\" family is CatBoost, developed by Yandex.\n",
    "\n",
    "The most powerful feature of this model, as its name suggests, is its ability to process **categorical** data automatically and very efficiently (while preventing data leakage).\n",
    "\n",
    "*Note: In our project, we had already converted all categorical data to numerical values using `Target Encoding` in Step 3. Therefore, we are not testing this core feature of CatBoost; we are feeding it our pre-processed numerical data, just like other models.*\n",
    "\n",
    "Nevertheless, CatBoost is known as a model that generally produces very stable and highly accurate results with default parameters (1000 trees).\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.9494\n",
    "  - **MAE:** 1366.16\n",
    "  - **RMSE:** 2201.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b58e6-f0d5-403f-a39c-01d35483bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostRegressor(random_state=7, silent=True)\n",
    "start_fit = time.time()\n",
    "model_cb.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_cb = model_cb.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_cb = r2_score(y_test, y_pred_cb)\n",
    "mae_cb = mean_absolute_error(y_test, y_pred_cb)\n",
    "mse_cb = mean_squared_error(y_test, y_pred_cb)\n",
    "rmse_cb = np.sqrt(mse_cb)\n",
    "print(\"\\n--- CatBoost Results ---\")\n",
    "print(f\"R-squared: {r2_cb:.4f}\")\n",
    "print(f\"MAE: {mae_cb:.2f} $\")\n",
    "print(f\"RMSE: {rmse_cb:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5113225-748b-4281-ba41-269b8e078f8f",
   "metadata": {},
   "source": [
    "### 5.9 Model 9: AdaBoost (Adaptive Boosting)\n",
    "\n",
    "The final model in our baseline comparison list is AdaBoost (Adaptive Boosting), the original progenitor of the \"boosting\" family.\n",
    "\n",
    "This model uses a different \"boosting\" logic than GBM or XGBoost. Instead of correcting errors with a \"gradient\", it assigns more **\"weight\"** to the data points that the previous model predicted *incorrectly*. This forces the next model to focus on these \"difficult\" examples.\n",
    "\n",
    "- Performance Results:\n",
    "  - **R²:** 0.1338\n",
    "  - **MAE:** 7901.81\n",
    "  - **RMSE:** 9111.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c4482-41eb-4cf4-b7b1-200073cd151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ada = AdaBoostRegressor(random_state=7)\n",
    "start_fit = time.time()\n",
    "model_ada.fit(x_train, y_train)\n",
    "end_fit = time.time()\n",
    "print(f\"The model was trained for {end_fit-start_fit:.2f} seconds.\")\n",
    "start_pred = time.time()\n",
    "y_pred_ada = model_ada.predict(x_test)\n",
    "end_pred = time.time()\n",
    "print(f\"The model predicted in {end_pred-start_pred:.2f} seconds.\")\n",
    "\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "mae_ada = mean_absolute_error(y_test, y_pred_ada)\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "rmse_ada = np.sqrt(mse_ada)\n",
    "print(\"\\n--- AdaBoost Results ---\")\n",
    "print(f\"R-squared: {r2_ada:.4f}\")\n",
    "print(f\"MAE: {mae_ada:.2f} $\")\n",
    "print(f\"RMSE: {rmse_ada:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74954178-e0e8-423b-b088-4ba59cd1cf5c",
   "metadata": {},
   "source": [
    "## Step 5.10: Baseline Comparison Results (Summary Table)\n",
    "\n",
    "Throughout Step 5, nine different machine learning models were trained with their **default parameters** and compared on the same test data. The purpose of this baseline comparison is to determine which algorithms are \"naturally\" suited to this dataset and which ones deserve to be \"finalists\" for optimisation (Step 6).\n",
    "\n",
    "All models were run on the *same* training and test data prepared in Step 3 (`Target Encoding`) and Step 4 (`StandardScaler`).\n",
    "\n",
    "### Analysis and Findings\n",
    "\n",
    "This table provides very clear insights for our project:\n",
    "\n",
    "1.  **Clear Winner (Accuracy):** **Random Forest** (RMSE: 2027.03) was by far the most successful model, delivering the lowest error with default parameters.\n",
    "2.  **Best Finalists:** **CatBoost** (RMSE: 2112.82) and **XGBoost** (RMSE: 2287.18) proved the power of the \"boosting\" family as the second and third best models.\n",
    "3.  **Speed Champion:** **LightGBM** became the fastest model by training 316,000 lines of data in under 1 second (0.87 seconds). It has tremendous potential for optimisation.\n",
    "4.  **Major Disappointments:** Classic `Gradient Boosting` (RMSE: 3404.94) and `AdaBoost` (RMSE: 9347.90) proved how far behind modern boosting implementations (XGB, LGBM, CatBoost) they are.\n",
    "5.  **Strong Starts:** Simpler models such as `Decision Tree` (RMSE: 2832.68) and `KNN` (RMSE: 3243.52) easily outperformed `Linear Regression` (RMSE: 4591.67) thanks to our features enhanced with `Target Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45435af-9f82-4760-9213-3e3497269081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"Model\": \"Linear Regression\", \"R²\": 0.7615, \"MAE ($)\": 3092.15, \"RMSE ($)\": 4780.8, \"Train Time (seconds)\": 0.19},\n",
    "    {\"Model\": \"KNN\", \"R²\": 0.8842, \"MAE ($)\": 2003.76, \"RMSE ($)\": 3331.8, \"Train Time (seconds)\": 2.28},\n",
    "    {\"Model\": \"Decision Tree\", \"R²\": 0.9077, \"MAE ($)\": 1728.58, \"RMSE ($)\": 2975.09, \"Train Time (seconds)\": 5.56},\n",
    "    {\"Model\": \"Random Forest\", \"R²\": 0.9551, \"MAE ($)\": 1220.07, \"RMSE ($)\": 2075.03, \"Train Time (seconds)\": 84.34},\n",
    "    {\"Model\": \"Gradient Boosting\", \"R²\": 0.8700, \"MAE ($)\": 2220.40, \"RMSE ($)\": 3530.08, \"Train Time (seconds)\": 96.05},\n",
    "    {\"Model\": \"XGBoost\", \"R²\": 0.9417, \"MAE ($)\": 1463.40, \"RMSE ($)\": 2363.51, \"Train Time (seconds)\": 1.67},\n",
    "    {\"Model\": \"LightGBM\", \"R²\": 0.9229, \"MAE ($)\": 1736.79, \"RMSE ($)\": 2718.66, \"Train Time (seconds)\": 1.69},\n",
    "    {\"Model\": \"CatBoost\", \"R²\": 0.9494, \"MAE ($)\": 1366.16, \"RMSE ($)\": 2201.53, \"Train Time (seconds)\": 34.16},\n",
    "    {\"Model\": \"AdaBoost\", \"R²\": 0.1338, \"MAE ($)\": 7901.81, \"RMSE ($)\": 9111.64, \"Train Time (seconds)\": 37.49}\n",
    "]\n",
    "results_df = pd.DataFrame(data)\n",
    "results_df = results_df.sort_values(by=\"RMSE ($)\", ascending=True)\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(\"model_baseline_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a76289-c3c5-4254-844b-1cdc699a5f02",
   "metadata": {},
   "source": [
    "## Step 6: Hyperparameter Optimisation (Finding the Best Parameters)\n",
    "\n",
    "Step 5's \"Basic Comparison\" showed the results obtained by the models using their default parameters. In this step, to find the \"best\" model, which is the main objective of the project, we will take the **top 3 finalists (Random Forest, CatBoost, XGBoost)** from that table and subject them to **Hyperparameter Optimisation** (Tuning).\n",
    "\n",
    "The goal is to reduce their error margins (RMSE) further by modifying these models' default settings (e.g., number of trees).\n",
    "\n",
    "For this process, `sklearn.model_selection.RandomisedSearchCV` was used. This tool finds the best settings by trying random combinations from the specified parameter ranges (`n_iter=10`) and testing them with 3-fold cross-validation (`cv=3`).\n",
    "\n",
    "### 6.1 Finalist 1: Random Forest Optimisation\n",
    "\n",
    "**Reason:** It led the baseline comparison with the lowest RMSE value (2,027.03).\n",
    "**Disadvantage:** As a single training run took 43 seconds, this optimisation process (10 iter * 3 CV) was very slow.\n",
    "\n",
    "* **Baseline RMSE:** 2075.03 \n",
    "* **Optimised RMSE:** 2072.57\n",
    "* **Best Parameters Found:**\n",
    "    * `n_estimators`: 100\n",
    "    * `max_features`: *1.0*\n",
    "    * `max_depth`: *none*\n",
    "    * `min_samples_split`: *5*\n",
    "    * `min_samples_leaf`: *1*\n",
    "* **Improvement:** *2.46* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787f330-89de-48f7-900b-e535d731a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300], # The number of trees in the forest\n",
    "    'max_depth': [10, 20, 30, None], # Maximum depth of trees (None=unlimited)\n",
    "    'max_features': ['sqrt', 'log2', 1.0], # Number of features to be used in each tree\n",
    "    'min_samples_split': [2, 5],      # Minimum sample required to split a branch\n",
    "    'min_samples_leaf': [1, 2, 4]       # Minimum sample required per leaf\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=7)\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10, \n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2 # Show progress while searching\n",
    ")\n",
    "start_time = time.time()\n",
    "rf_random_search.fit(x_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Optimisation completed. Duration: {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best Parameters Found:\\n {rf_random_search.best_params_}\")\n",
    "best_rf = rf_random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(x_test)\n",
    "\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "rmse_best_rf = np.sqrt(mean_squared_error(y_test, y_pred_best_rf))\n",
    "print(f\"Optimised R-Squared: {r2_best_rf:.4f}\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_rf:.2f} $\")\n",
    "\n",
    "print(f\"Baseline RMSE:       {2027.03:.2f} $\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_rf:.2f} $\")\n",
    "print(f\"Recovery (RMSE):     {2027.03 - rmse_best_rf:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6213c2-a61b-4ded-9067-76914c3e27ca",
   "metadata": {},
   "source": [
    "### 6.2 Finalist 2: CatBoost Optimisation\n",
    "\n",
    "**Reason:** It was a very strong \"boosting\" candidate, having the second lowest RMSE (2112.82) in the baseline comparison. Unlike Random Forest, CatBoost is generally expected to respond very well to optimisation.\n",
    "\n",
    "* **Baseline RMSE:** 2201.53 \n",
    "* **Optimised RMSE:** *2108.06* \n",
    "* **Best Parameters Found:**\n",
    "    * `iterations`: *1000*\n",
    "    * `learning_rate`: *0.05*\n",
    "    * `depth`: *10*\n",
    "    * `l2_leaf_reg`: *5*\n",
    "* **Improvement:** *93.47* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618f818-257f-482d-a953-b85fb8480d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'iterations': [500, 1000, 1500],  # Number of trees (baseline was 1000)\n",
    "    'depth': [6, 8, 10],              # Tree depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1], # Learning speed (one of the most important ones)\n",
    "    'l2_leaf_reg': [1, 3, 5]          # The L2 penalty term that prevents overfitting\n",
    "}\n",
    "cb = CatBoostRegressor(random_state=7, silent=True)\n",
    "cb_random_search = RandomizedSearchCV(\n",
    "    estimator=cb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10, \n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2 \n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "cb_random_search.fit(x_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Optimisation completed. Duration: {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best Parameters Found:\\n {cb_random_search.best_params_}\")\n",
    "best_cb = cb_random_search.best_estimator_\n",
    "y_pred_best_cb = best_cb.predict(x_test)\n",
    "\n",
    "r2_best_cb = r2_score(y_test, y_pred_best_cb)\n",
    "rmse_best_cb = np.sqrt(mean_squared_error(y_test, y_pred_best_cb))\n",
    "print(f\"Optimised R-Squared: {r2_best_cb:.4f}\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_cb:.2f} $\")\n",
    "\n",
    "print(f\"Baseline RMSE:       {2112.82:.2f} $\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_cb:.2f} $\")\n",
    "print(f\"Recovery (RMSE):     {2112.82 - rmse_best_cb:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf2dc1-b8b4-446c-bec7-cb6bec3225fb",
   "metadata": {},
   "source": [
    "### 6.3 Finalist 3: XGBoost Optimisation\n",
    "\n",
    "**Reason:** It had the third-best RMSE (2,287.18) in the baseline comparison. However, its biggest advantage was its **speed**; it was trained in 1.35 seconds. This offers the potential to make the optimisation (tuning) process extremely efficient.\n",
    "\n",
    "* **Baseline RMSE:** 2363.51\n",
    "* **Optimised RMSE:** 1,902.00\n",
    "* **Best Found Parameters:**\n",
    "    * `subsample`: *1.0*\n",
    "    * `n_estimators`: *1000*\n",
    "    * `max_depth`: *7*\n",
    "    * `learning_rate`: *0.1*\n",
    "    * `gamma`: *0.1*\n",
    "* **Improvement:** *461.51* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28d9af-8a94-43c9-915e-0964a47b9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 300, 500, 1000], # Number of trees\n",
    "    'max_depth': [3, 5, 7, 9],          # Tree depth\n",
    "    'learning_rate': [0.01, 0.05, 0.1], # Learning speed\n",
    "    'subsample': [0.7, 0.8, 1.0],       # How much of the data will be used for each tree?\n",
    "    'gamma': [0, 0.1, 0.2]              # Penalty term preventing overfitting\n",
    "}\n",
    "xgb = XGBRegressor(random_state=7, n_jobs=-1)\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10, \n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_random_search.fit(x_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Optimisation completed. Duration: {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best Parameters Found:\\n {xgb_random_search.best_params_}\")\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(x_test)\n",
    "\n",
    "r2_best_xgb = r2_score(y_test, y_pred_best_xgb)\n",
    "rmse_best_xgb = np.sqrt(mean_squared_error(y_test, y_pred_best_xgb))\n",
    "print(f\"Optimised R-Squared: {r2_best_xgb:.4f}\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_xgb:.2f} $\")\n",
    "\n",
    "print(f\"Baseline RMSE:       {2287.18:.2f} $\")\n",
    "print(f\"Optimised RMSE:      {rmse_best_xgb:.2f} $\")\n",
    "print(f\"Recovery (RMSE):     {2287.18 - rmse_best_xgb:.2f} $\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb13c3-6563-45d6-bd88-356b6a344143",
   "metadata": {},
   "source": [
    "### 6.4 Validating the Final Model: Overfitting Test\n",
    "\n",
    "In Step 6.3, we found that the **Optimised XGBoost** model gave the best *test* score with an RMSE of 1800.91.\n",
    "\n",
    "However, a model being \"best\" means not only having a low test score, but also being *robust*. In this step, a validation test was performed to check whether the model \"memorised\" the training data (overfitting).\n",
    "\n",
    "The model was trained with the best parameters (`n_estimators=1000`, `max_depth=7`, etc.) and made predictions on both the *training data* (seen before) and the *test data* (never seen before).\n",
    "\n",
    "* **Training RMSE:** *1565.98* \n",
    "* **Test RMSE:** *1902.00* \n",
    "* **Difference (Test - Training):** *336.01* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1387b-62f0-4c8e-848b-22c736f78f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {\n",
    "    'subsample': 1.0, \n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 7, \n",
    "    'learning_rate': 0.1, \n",
    "    'gamma': 0.1,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1         \n",
    "}\n",
    "model_xgb_final = XGBRegressor(**best_params_xgb)\n",
    "model_xgb_final.fit(x_train, y_train)\n",
    "print(\"... Predictions are being made on the test data....\")\n",
    "y_pred_test = model_xgb_final.predict(x_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"... Predictions are being made on training data...\")\n",
    "y_pred_train = model_xgb_final.predict(x_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"Train RMSE: {rmse_train:.2f} $\")\n",
    "print(f\"Test RMSE:  {rmse_test:.2f} $\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(f\"Difference (Test - Train):         {rmse_test - rmse_train:.2f} $\")\n",
    "print(f\"Test Error / Training Error Ratio: {rmse_test / rmse_train:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197cf095-e7f1-4bca-b6f7-142d43509e5b",
   "metadata": {},
   "source": [
    "### 6.5 Visual Validation of the Final Model: Learning Curve\n",
    "\n",
    "The best way to prove how \"healthy\" the model (Optimised XGBoost) is, is to visualise the training process.\n",
    "\n",
    "The graph below shows how the error (RMSE) on both the Training and Test data changed *each time a tree was added* throughout the model's 1000-tree (`n_estimators`) training process.\n",
    "\n",
    "<center><img src=\"xgboost_learning_curve.png\" alt=\"Drawing\" style=\"width\": 900px;/></center>\n",
    "\n",
    "**Graph Analysis:**\n",
    "\n",
    "* **Training RMSE Line (Blue):** Shows that the model reduces its error (as expected) to 1391 as it sees the training data. The fact that it does not drop to zero (overfitting) is a healthy sign.\n",
    "* **Test RMSE Line (Orange):** This is the model's error on data it has never seen before. This line also decreases *parallel* to the training line and reaches a **stable (flat) plateau** at around 1800 after approximately 200-300 trees.\n",
    "\n",
    "**Conclusion:** The fact that the test line does not rise while the training line falls, but instead follows it in a parallel and stable manner, visually demonstrates that our model is not overfitting and is successfully generalising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23605f01-d308-44dc-b5c8-18209c6e70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {\n",
    "    'subsample': 1.0, \n",
    "    'n_estimators': 1000, \n",
    "    'max_depth': 7, \n",
    "    'learning_rate': 0.1, \n",
    "    'gamma': 0.1,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1\n",
    "}\n",
    "model_xgb_final = XGBRegressor(**best_params_xgb)\n",
    "eval_set = [(x_train, y_train), (x_test, y_test)]\n",
    "model_xgb_final.fit(x_train, y_train, eval_set=eval_set, verbose=False)\n",
    "results = model_xgb_final.evals_result()\n",
    "train_rmse = results['validation_0']['rmse']\n",
    "test_rmse = results['validation_1']['rmse']\n",
    "iterations = range(1, len(train_rmse) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.lineplot(x=iterations, y=train_rmse, label='Train RMSE')\n",
    "sns.lineplot(x=iterations, y=test_rmse, label='Test RMSE')\n",
    "plt.title('XGBoost Learning Curve\\n(Overfitting Test)', fontsize=16)\n",
    "plt.xlabel('Number of Trees (Iteration)', fontsize=12)\n",
    "plt.ylabel('RMSE', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.ylim(bottom=0, top=min(max(train_rmse), max(test_rmse)) * 1.1) \n",
    "plt.savefig(\"xgboost_learning_curve.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea68e4-2698-4b4a-80a0-e6821e5f8210",
   "metadata": {},
   "source": [
    "## Step 7: Final Results and Project Summary\n",
    "\n",
    "This project aimed to solve a vehicle price prediction problem using 9 different machine learning models (`car_prices.csv`).\n",
    "\n",
    "The project flow consisted of the following steps:\n",
    "1.  **Step 1: Exploratory Data Analysis (EDA)** (Visual and numerical analysis)\n",
    "2.  **Step 2: Data Cleaning and Feature Engineering** (`car_age` was created, `mmr` leakage was prevented)\n",
    "3.  **Step 3: Advanced Encoding** (`Target Encoding` was used for high-cardinality columns such as `model` and `trim`)\n",
    "4.  **Step 4: Feature Scaling** (`StandardScaler` applied for KNN and LinReg)\n",
    "5.  **Step 5: Baseline Comparison** (9 models tested with default parameters and **Random Forest** (RMSE: $2027) selected as leader)\n",
    "6.  **Step 6: Hyperparameter Optimisation** (The top 3 finalists - RF, CatBoost, XGBoost - were optimised using `RandomisedSearchCV`)\n",
    "\n",
    "### The Project's Final Winner\n",
    "\n",
    "The optimisation tests conducted in Step 6 altered the baseline comparison results from Step 5 and determined the project's final winner.\n",
    "\n",
    "* **Random Forest** optimisation failed due to memory errors and overfitting (RMSE: 2072.57), failing to surpass the default model (RMSE: 2027.03).\n",
    "* **CatBoost** optimisation took 10.04 minutes and took the lead by reducing the RMSE from 2112.82 to **2108.06**.\n",
    "* **XGBoost** optimisation took only **2.41 minutes** and achieved an **overwhelming superiority** by reducing the RMSE from 2287$ to **1902.00$**.\n",
    "\n",
    "### FINAL WINNING TABLE (Based on RMSE)\n",
    "\n",
    "| Model | Status | RMSE ($) | R² | Optimisation Time (Minutes) | Description |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| **XGBoost (Optimised)** | **Project Winner** | **1902.00** | **0.9623** | **2.41** | **Best balance of speed and accuracy.** |\n",
    "| CatBoost (Optimised) | Second | 2108.06 | 0.9536 | 10.04 | Very strong and stable accuracy. |\n",
    "| Random Forest (Baseline) | Third | 2072.57 | 0.9552 | 21.15 | Optimisation failed (Overfitting). |\n",
    "\n",
    "### Project Result\n",
    "\n",
    "In this project, 9 models were compared, and the **Optimised XGBoost** model was selected as the best price prediction model with an RMSE of £1902.00 and an R² score of 96.23%.\n",
    "\n",
    "(The project's best parameters: `{'subsample': 1.0, 'n_estimators': 1000, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0.1}`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['make', 'model', 'trim', 'body', 'transmission', 'color', 'interior', 'seller']\n",
    "for col in categorical_cols:\n",
    "\n",
    "    df[col] = df[col].astype(str).str.title().str.strip()\n",
    "\n",
    "df['state'] = df['state'].str.upper().str.strip()\n",
    "\n",
    "us_states = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "state_ui_map = {us_states.get(code, code): code for code in sorted(df['state'].unique())}\n",
    "\n",
    "options_optimized = {\n",
    "    \"makes\": sorted(df['make'].unique().tolist()),\n",
    "    \"transmissions\": sorted(df['transmission'].unique().tolist()),\n",
    "    \"colors\": sorted(df['color'].unique().tolist()),\n",
    "    \"interiors\": sorted(df['interior'].unique().tolist()),\n",
    "    \"states_map\": state_ui_map,  \n",
    "    \"make_models\": df.groupby('make')['model'].unique().apply(lambda x: sorted(list(x))).to_dict(),\n",
    "    \"model_trims\": df.groupby('model')['trim'].unique().apply(lambda x: sorted(list(x))).to_dict(),\n",
    "    \"model_bodies\": df.groupby('model')['body'].unique().apply(lambda x: sorted(list(x))).to_dict(),\n",
    "    \"make_sellers\": df.groupby('make')['seller'].unique().apply(lambda x: sorted(list(x))).to_dict()\n",
    "}\n",
    "\n",
    "with open('options.json', 'w') as f:\n",
    "    json.dump(options_optimized, f)\n",
    "\n",
    "joblib.dump(model_xgb_final, 'xgb_model.joblib')\n",
    "joblib.dump(encoder, 'target_encoder.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3db025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
